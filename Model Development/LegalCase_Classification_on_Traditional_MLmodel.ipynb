{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "942ef65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "nlp = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a9d598d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, confusion_matrix, recall_score, precision_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "799cc263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "928b6ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "32723da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = pd.read_csv('cleaned_legal_case.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "97896368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>DocumentId</th>\n",
       "      <th>Postures</th>\n",
       "      <th>HeadText</th>\n",
       "      <th>Paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>31944</td>\n",
       "      <td>On Appeal</td>\n",
       "      <td>other</td>\n",
       "      <td>Plaintiff Dwight Watson (“Husband”) appeals fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>31944</td>\n",
       "      <td>On Appeal</td>\n",
       "      <td>facts</td>\n",
       "      <td>Husband and Wife were married in November 1989...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>31944</td>\n",
       "      <td>On Appeal</td>\n",
       "      <td>reasoning/analysis</td>\n",
       "      <td>Husband argues that the trial court erred in v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>31944</td>\n",
       "      <td>On Appeal</td>\n",
       "      <td>issues</td>\n",
       "      <td>Although Husband does not clearly identify an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>31944</td>\n",
       "      <td>On Appeal</td>\n",
       "      <td>other</td>\n",
       "      <td>Husband contends that the trial court’s findin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39924</th>\n",
       "      <td>39924</td>\n",
       "      <td>39930</td>\n",
       "      <td>Motion to Compel Arbitration</td>\n",
       "      <td>other</td>\n",
       "      <td>Because Romero’s STELA claim is governed by th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39925</th>\n",
       "      <td>39925</td>\n",
       "      <td>39931</td>\n",
       "      <td>Motion for Attorney's Fees</td>\n",
       "      <td>other</td>\n",
       "      <td>The legal question at the core of this appeal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39926</th>\n",
       "      <td>39926</td>\n",
       "      <td>39932</td>\n",
       "      <td>Motion to Dismiss</td>\n",
       "      <td>other</td>\n",
       "      <td>Order and judgment (one paper), Supreme Court,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39927</th>\n",
       "      <td>39927</td>\n",
       "      <td>39933</td>\n",
       "      <td>Motion for Relief from Order or Judgment</td>\n",
       "      <td>other</td>\n",
       "      <td>U.S. Bank National Association (USBNA) appeals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39928</th>\n",
       "      <td>39928</td>\n",
       "      <td>39934</td>\n",
       "      <td>Motion for Default Judgment/Order of Default</td>\n",
       "      <td>other</td>\n",
       "      <td>In an action to foreclose a mortgage, the defe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39929 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  DocumentId                                      Postures  \\\n",
       "0               0       31944                                     On Appeal   \n",
       "1               1       31944                                     On Appeal   \n",
       "2               2       31944                                     On Appeal   \n",
       "3               3       31944                                     On Appeal   \n",
       "4               4       31944                                     On Appeal   \n",
       "...           ...         ...                                           ...   \n",
       "39924       39924       39930                  Motion to Compel Arbitration   \n",
       "39925       39925       39931                    Motion for Attorney's Fees   \n",
       "39926       39926       39932                             Motion to Dismiss   \n",
       "39927       39927       39933      Motion for Relief from Order or Judgment   \n",
       "39928       39928       39934  Motion for Default Judgment/Order of Default   \n",
       "\n",
       "                 HeadText                                          Paragraph  \n",
       "0                   other  Plaintiff Dwight Watson (“Husband”) appeals fr...  \n",
       "1                   facts  Husband and Wife were married in November 1989...  \n",
       "2      reasoning/analysis  Husband argues that the trial court erred in v...  \n",
       "3                  issues  Although Husband does not clearly identify an ...  \n",
       "4                   other  Husband contends that the trial court’s findin...  \n",
       "...                   ...                                                ...  \n",
       "39924               other  Because Romero’s STELA claim is governed by th...  \n",
       "39925               other  The legal question at the core of this appeal ...  \n",
       "39926               other  Order and judgment (one paper), Supreme Court,...  \n",
       "39927               other  U.S. Bank National Association (USBNA) appeals...  \n",
       "39928               other  In an action to foreclose a mortgage, the defe...  \n",
       "\n",
       "[39929 rows x 5 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9dfb62b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_case = doc.loc[(doc['Paragraph'].notna()) & (doc['HeadText'] != 'other'),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "67f26dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "facts                 3223\n",
       "oder and decision     2435\n",
       "reasoning/analysis    2211\n",
       "issues                1322\n",
       "rules                 1126\n",
       "procedural history     818\n",
       "Name: HeadText, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legal_case.HeadText.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "590a0027",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_duplicate_df = legal_case.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bf2b7d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11135, 5)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_duplicate_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4dea0f",
   "metadata": {},
   "source": [
    "# Convert text to vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b739e3",
   "metadata": {},
   "source": [
    "# revmove stopwords from the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e290be01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords from the documents\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "90ecf00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "myFile = open('cleaned_documents_stopwords.txt','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "487f74ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for line in myFile:\n",
    "    documents.append(line.rstrip('\\n'))\n",
    "myFile.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4e3e66bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11135"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501ed28d",
   "metadata": {},
   "source": [
    "# a. Word2Vec Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b11c3e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CINDY~1.DES\\AppData\\Local\\Temp/ipykernel_6496/4210083686.py:12: DeprecationWarning: Call to deprecated `init_sims` (Use fill_norms() instead. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  nlp.init_sims(replace=True) # calling for using syn0norm\n"
     ]
    }
   ],
   "source": [
    "# Download and import essential libraries\n",
    "#!pip install stop-words\n",
    "#from stop_words import get_stop_words\n",
    "#stopwords = get_stop_words('en')\n",
    "# from textblob import Word\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "import logging\n",
    "\n",
    "nlp.init_sims(replace=True) # calling for using syn0norm\n",
    "\n",
    "\n",
    "# Tokenizing the document text, return a list of word for each document\n",
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "# Defining a function to get pretrained embedding for each token\n",
    "# and take an average of word2vec embeddings of all tokens in the document \n",
    "# as vector representation for that document.\n",
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv:\n",
    "            mean.append(wv.vectors[wv.key_to_index[word]]) # appending each 300-D embedding for each word\n",
    "            all_words.add(wv.key_to_index[word])\n",
    "    if not mean:\n",
    "        logging.warning(\"no input %s\", words)\n",
    "        return np.zeros(wv.vector_size,)\n",
    "    # computing the mean of the embedding list\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "# Defining a function to stack together all document embeddings into one\n",
    "def  word_averaging_list(wv, text_list):\n",
    "   stack = np.vstack([word_averaging(wv, post) for post in text_list ])\n",
    "   return stack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "10600ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tokens = []\n",
    "for doc in documents:\n",
    "    x_tokens.append(w2v_tokenize_text(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0fc74282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:no input []\n",
      "WARNING:root:no input ['mcl']\n"
     ]
    }
   ],
   "source": [
    "x_w2v_vectors = word_averaging_list(nlp,x_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3a113345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11135, 300)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_w2v_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0995ce47",
   "metadata": {},
   "source": [
    "# b. TF-IDF Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c9afb925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def get_tfidf_vectors(x):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(x)\n",
    "    #feature_name = vectorizer.get_feature_names()\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    x_tfidf_vector = np.array(denselist)\n",
    "    return x_tfidf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "335f1434",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tfidf_vectors = get_tfidf_vectors(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2ed822f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11135, 27264)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_tfidf_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9787f088",
   "metadata": {},
   "source": [
    "# Run and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a45d1fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_performance_metric(actual_y,predict_y):\n",
    "    accuracy_on_test_dataset = accuracy_score(actual_y, predict_y)\n",
    "    f1_score_test_dataset = f1_score(actual_y, predict_y, average='macro',zero_division=1)\n",
    "    recall_score_test_dataset = recall_score(actual_y, predict_y, average = 'macro', zero_division=1)\n",
    "    precision_score_test_dataset = precision_score(actual_y, predict_y, average ='macro', zero_division=1)\n",
    "    confu_matrix = confusion_matrix(actual_y, predict_y)\n",
    "    classify_report = classification_report(actual_y, predict_y)\n",
    "    print('Accuracy score on the test dataset: ',accuracy_on_test_dataset)\n",
    "    print('Recall score on the test dataset:', recall_score_test_dataset)\n",
    "    print('Precision score on the test dataset:', precision_score_test_dataset)\n",
    "    print('F1 score on the test dataset:', f1_score_test_dataset)\n",
    "    print(confu_matrix)\n",
    "    print(classify_report)\n",
    "    return accuracy_on_test_dataset, f1_score_test_dataset,recall_score_test_dataset,precision_score_test_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2566eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_score(name, model,x_train_vectors,x_test_vectors,y_train,y_test):\n",
    "    cv = StratifiedKFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "    scores = []\n",
    "    f1_scores = []\n",
    "    oversampler = SMOTE(random_state=42)\n",
    "    for train_fold_index, val_fold_index in cv.split(x_train_vectors,y_train):\n",
    "        x_train_fold = x_train_vectors[train_fold_index]\n",
    "        y_train_fold = y_train.iloc[train_fold_index]\n",
    "        x_val_fold = x_train_vectors[val_fold_index]\n",
    "        y_val_fold = y_train.iloc[val_fold_index]\n",
    "\n",
    "        #upsample on the training dataset\n",
    "        X_train_fold_upsample, y_train_fold_upsample = oversampler.fit_resample(x_train_fold,y_train_fold)\n",
    "        model.fit(X_train_fold_upsample,y_train_fold_upsample)\n",
    "        y_predict = model.predict(x_val_fold)\n",
    "        score = accuracy_score(y_val_fold,y_predict)\n",
    "        scores.append(score)\n",
    "        f1 = f1_score(y_val_fold, y_predict,  average = 'macro', zero_division=1)\n",
    "        f1_scores.append(f1)\n",
    "        \n",
    "    #validate on the test dataset\n",
    "    y_test_predict = model.predict(x_test_vectors)\n",
    "    accuracy_test,f1_score_test,recall_score_test,precision_score_test = print_performance_metric(y_test,y_test_predict)\n",
    "    \n",
    "    \n",
    "    report_score = {'macro_f1_folds':np.array(f1_scores), 'accuracy_test':accuracy_test,'macro_f1_test':f1_score_test,\n",
    "                   'recall_score_test':recall_score_test,'precision_score_test':precision_score_test}\n",
    "    return report_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "80581a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to split the dataset\n",
    "def split_train_test(x_vectors, labels):\n",
    "    X = x_vectors #list type\n",
    "    y = labels #Seires type\n",
    "    X_train, X_test, y_train, y_test = train_test_split (X, y, train_size = 0.8, random_state = 42, shuffle = True, stratify=y)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2bcaf031",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('LR', LogisticRegression(solver='lbfgs', max_iter=700)))\n",
    "models.append(('RF', RandomForestClassifier()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('DT', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(decision_function_shape='ovo', probability=True)))\n",
    "models.append(('LGBM', LGBMClassifier(objective= 'binary')))\n",
    "models.append(('XGB', XGBClassifier(eval_metric=\"mlogloss\", objective = \"reg:logistic\") ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea0d341",
   "metadata": {},
   "source": [
    "# Run and train model on w2v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "874403d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v_train, X_w2v_test,y_w2v_train,y_w2v_test = split_train_test(x_w2v_vectors,drop_duplicate_df['HeadText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "97a3e377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on the test dataset:  0.590929501571621\n",
      "Recall score on the test dataset: 0.5855499218842913\n",
      "Precision score on the test dataset: 0.5607988501434482\n",
      "F1 score on the test dataset: 0.5620150398977694\n",
      "[[321  11  19 192  56  46]\n",
      " [  4 191   6   3  30  30]\n",
      " [ 12  12 399   7  42  15]\n",
      " [ 66   2   6  74   7   9]\n",
      " [ 32  70  33  15 206  86]\n",
      " [ 16  17   9  10  48 125]]\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "             facts       0.71      0.50      0.59       645\n",
      "            issues       0.63      0.72      0.67       264\n",
      " oder and decision       0.85      0.82      0.83       487\n",
      "procedural history       0.25      0.45      0.32       164\n",
      "reasoning/analysis       0.53      0.47      0.50       442\n",
      "             rules       0.40      0.56      0.47       225\n",
      "\n",
      "          accuracy                           0.59      2227\n",
      "         macro avg       0.56      0.59      0.56      2227\n",
      "      weighted avg       0.63      0.59      0.60      2227\n",
      "\n",
      "LR : 0.5516741388197459 (0.01005573859616252)\n",
      "Accuracy score on the test dataset:  0.5954198473282443\n",
      "Recall score on the test dataset: 0.5211907290907565\n",
      "Precision score on the test dataset: 0.5452704138587413\n",
      "F1 score on the test dataset: 0.5268275798802475\n",
      "[[478  12  26  42  63  24]\n",
      " [ 13 165  12   0  53  21]\n",
      " [ 24   9 366   1  77  10]\n",
      " [111   2   8  29   7   7]\n",
      " [ 67  58  47   5 205  60]\n",
      " [ 41  11   8   2  80  83]]\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "             facts       0.65      0.74      0.69       645\n",
      "            issues       0.64      0.62      0.63       264\n",
      " oder and decision       0.78      0.75      0.77       487\n",
      "procedural history       0.37      0.18      0.24       164\n",
      "reasoning/analysis       0.42      0.46      0.44       442\n",
      "             rules       0.40      0.37      0.39       225\n",
      "\n",
      "          accuracy                           0.60      2227\n",
      "         macro avg       0.55      0.52      0.53      2227\n",
      "      weighted avg       0.59      0.60      0.59      2227\n",
      "\n",
      "RF : 0.5095626690519387 (0.015361621655784873)\n",
      "Accuracy score on the test dataset:  0.42703188145487203\n",
      "Recall score on the test dataset: 0.5062602726726508\n",
      "Precision score on the test dataset: 0.49120552111622257\n",
      "F1 score on the test dataset: 0.4208857224189824\n",
      "[[ 98  41  21 337  51  97]\n",
      " [  1 185   3   4  34  37]\n",
      " [  4  61 300  26  42  54]\n",
      " [ 12   3   4 122   8  15]\n",
      " [  8 122  24  27 123 138]\n",
      " [  3  32   3  28  36 123]]\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "             facts       0.78      0.15      0.25       645\n",
      "            issues       0.42      0.70      0.52       264\n",
      " oder and decision       0.85      0.62      0.71       487\n",
      "procedural history       0.22      0.74      0.34       164\n",
      "reasoning/analysis       0.42      0.28      0.33       442\n",
      "             rules       0.27      0.55      0.36       225\n",
      "\n",
      "          accuracy                           0.43      2227\n",
      "         macro avg       0.49      0.51      0.42      2227\n",
      "      weighted avg       0.59      0.43      0.42      2227\n",
      "\n",
      "KNN : 0.40936116618114965 (0.004836289182381326)\n",
      "Accuracy score on the test dataset:  0.3762909744050292\n",
      "Recall score on the test dataset: 0.34995917237201574\n",
      "Precision score on the test dataset: 0.3443902587792622\n",
      "F1 score on the test dataset: 0.34364356733588086\n",
      "[[252  40  63 142  97  51]\n",
      " [ 29  96  37  13  50  39]\n",
      " [ 54  37 256  20  83  37]\n",
      " [ 74   8  13  41  13  15]\n",
      " [ 47  65  90  27 132  81]\n",
      " [ 40  31  23  14  56  61]]\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "             facts       0.51      0.39      0.44       645\n",
      "            issues       0.35      0.36      0.35       264\n",
      " oder and decision       0.53      0.53      0.53       487\n",
      "procedural history       0.16      0.25      0.19       164\n",
      "reasoning/analysis       0.31      0.30      0.30       442\n",
      "             rules       0.21      0.27      0.24       225\n",
      "\n",
      "          accuracy                           0.38      2227\n",
      "         macro avg       0.34      0.35      0.34      2227\n",
      "      weighted avg       0.40      0.38      0.38      2227\n",
      "\n",
      "DT : 0.35068847565627853 (0.01137343346510587)\n",
      "Accuracy score on the test dataset:  0.5024696901661428\n",
      "Recall score on the test dataset: 0.4866511790186487\n",
      "Precision score on the test dataset: 0.46844054115720707\n",
      "F1 score on the test dataset: 0.4717696103929196\n",
      "[[322  20  37 158  51  57]\n",
      " [ 18 162  14   3  34  33]\n",
      " [ 30  20 336   3  69  29]\n",
      " [ 74   2  14  58   7   9]\n",
      " [ 68  73  48   9 141 103]\n",
      " [ 42  22   5   7  49 100]]\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "             facts       0.58      0.50      0.54       645\n",
      "            issues       0.54      0.61      0.58       264\n",
      " oder and decision       0.74      0.69      0.71       487\n",
      "procedural history       0.24      0.35      0.29       164\n",
      "reasoning/analysis       0.40      0.32      0.36       442\n",
      "             rules       0.30      0.44      0.36       225\n",
      "\n",
      "          accuracy                           0.50      2227\n",
      "         macro avg       0.47      0.49      0.47      2227\n",
      "      weighted avg       0.52      0.50      0.51      2227\n",
      "\n",
      "NB : 0.4679923879624523 (0.005667920674474106)\n",
      "Accuracy score on the test dataset:  0.6457117198024248\n",
      "Recall score on the test dataset: 0.5810019057458211\n",
      "Precision score on the test dataset: 0.5886003089437681\n",
      "F1 score on the test dataset: 0.5818636580618868\n",
      "[[478   4  15  55  59  34]\n",
      " [  8 186   5   0  39  26]\n",
      " [ 18   9 399   2  50   9]\n",
      " [111   1   6  38   4   4]\n",
      " [ 41  64  28   6 233  70]\n",
      " [ 22  15   9   3  72 104]]\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "             facts       0.71      0.74      0.72       645\n",
      "            issues       0.67      0.70      0.69       264\n",
      " oder and decision       0.86      0.82      0.84       487\n",
      "procedural history       0.37      0.23      0.28       164\n",
      "reasoning/analysis       0.51      0.53      0.52       442\n",
      "             rules       0.42      0.46      0.44       225\n",
      "\n",
      "          accuracy                           0.65      2227\n",
      "         macro avg       0.59      0.58      0.58      2227\n",
      "      weighted avg       0.64      0.65      0.64      2227\n",
      "\n",
      "SVM : 0.5803135051876793 (0.008519599575725022)\n",
      "Accuracy score on the test dataset:  0.6120341266277504\n",
      "Recall score on the test dataset: 0.5339954442463696\n",
      "Precision score on the test dataset: 0.5567101855007813\n",
      "F1 score on the test dataset: 0.5401368880977374\n",
      "[[480   3  23  47  68  24]\n",
      " [ 10 166   7   1  56  24]\n",
      " [ 28  11 376   0  61  11]\n",
      " [112   1   6  30   6   9]\n",
      " [ 51  57  35   5 232  62]\n",
      " [ 35  11   8   4  88  79]]\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "             facts       0.67      0.74      0.71       645\n",
      "            issues       0.67      0.63      0.65       264\n",
      " oder and decision       0.83      0.77      0.80       487\n",
      "procedural history       0.34      0.18      0.24       164\n",
      "reasoning/analysis       0.45      0.52      0.49       442\n",
      "             rules       0.38      0.35      0.36       225\n",
      "\n",
      "          accuracy                           0.61      2227\n",
      "         macro avg       0.56      0.53      0.54      2227\n",
      "      weighted avg       0.61      0.61      0.61      2227\n",
      "\n",
      "LGBM : 0.5493221463907141 (0.013253986782859451)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cindy.DESKTOP-P0UU59K\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\cindy.DESKTOP-P0UU59K\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\cindy.DESKTOP-P0UU59K\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\cindy.DESKTOP-P0UU59K\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\cindy.DESKTOP-P0UU59K\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on the test dataset:  0.608890884598114\n",
      "Recall score on the test dataset: 0.5368928386415034\n",
      "Precision score on the test dataset: 0.5648767455806984\n",
      "F1 score on the test dataset: 0.5453351392513166\n",
      "[[473   7  15  46  70  34]\n",
      " [ 13 162   7   1  58  23]\n",
      " [ 33  11 374   2  56  11]\n",
      " [107   0   5  36   7   9]\n",
      " [ 56  52  37   3 227  67]\n",
      " [ 37   8   9   2  85  84]]\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "             facts       0.66      0.73      0.69       645\n",
      "            issues       0.68      0.61      0.64       264\n",
      " oder and decision       0.84      0.77      0.80       487\n",
      "procedural history       0.40      0.22      0.28       164\n",
      "reasoning/analysis       0.45      0.51      0.48       442\n",
      "             rules       0.37      0.37      0.37       225\n",
      "\n",
      "          accuracy                           0.61      2227\n",
      "         macro avg       0.56      0.54      0.55      2227\n",
      "      weighted avg       0.61      0.61      0.61      2227\n",
      "\n",
      "XGB : 0.5443801805351182 (0.010800255953737858)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f1_score_folds = {}\n",
    "all_model_reports = {}\n",
    "\n",
    "for name, model in models:\n",
    "    \n",
    "    report = model_score(name, model,X_w2v_train, X_w2v_test,y_w2v_train,y_w2v_test) \n",
    "    f1_score_folds[name] = report['macro_f1_folds']\n",
    "   \n",
    "    all_model_reports[name] = report\n",
    "    print(\"{name} : {f1_folds_mean} ({std})\".format(name = name,f1_folds_mean =report['macro_f1_folds'].mean(),\n",
    "                                                   std = report['macro_f1_folds'].std()))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bd249901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LR': {'macro_f1_folds': array([0.54100735, 0.53965206, 0.55646264, 0.56628724, 0.5549614 ]),\n",
       "  'accuracy_test': 0.590929501571621,\n",
       "  'macro_f1_test': 0.5620150398977694,\n",
       "  'recall_score_test': 0.5855499218842913,\n",
       "  'precision_score_test': 0.5607988501434482},\n",
       " 'RF': {'macro_f1_folds': array([0.49033496, 0.4930537 , 0.52913921, 0.52109453, 0.51419095]),\n",
       "  'accuracy_test': 0.5954198473282443,\n",
       "  'macro_f1_test': 0.5268275798802475,\n",
       "  'recall_score_test': 0.5211907290907565,\n",
       "  'precision_score_test': 0.5452704138587413},\n",
       " 'KNN': {'macro_f1_folds': array([0.41102845, 0.40259654, 0.41512126, 0.41321369, 0.40484589]),\n",
       "  'accuracy_test': 0.42703188145487203,\n",
       "  'macro_f1_test': 0.4208857224189824,\n",
       "  'recall_score_test': 0.5062602726726508,\n",
       "  'precision_score_test': 0.49120552111622257},\n",
       " 'DT': {'macro_f1_folds': array([0.345109  , 0.33465125, 0.34863344, 0.36860928, 0.35643941]),\n",
       "  'accuracy_test': 0.3762909744050292,\n",
       "  'macro_f1_test': 0.34364356733588086,\n",
       "  'recall_score_test': 0.34995917237201574,\n",
       "  'precision_score_test': 0.3443902587792622},\n",
       " 'NB': {'macro_f1_folds': array([0.47054282, 0.46373044, 0.46355856, 0.47804418, 0.46408594]),\n",
       "  'accuracy_test': 0.5024696901661428,\n",
       "  'macro_f1_test': 0.4717696103929196,\n",
       "  'recall_score_test': 0.4866511790186487,\n",
       "  'precision_score_test': 0.46844054115720707},\n",
       " 'SVM': {'macro_f1_folds': array([0.58404049, 0.57177323, 0.59508758, 0.57735911, 0.57330711]),\n",
       "  'accuracy_test': 0.6457117198024248,\n",
       "  'macro_f1_test': 0.5818636580618868,\n",
       "  'recall_score_test': 0.5810019057458211,\n",
       "  'precision_score_test': 0.5886003089437681},\n",
       " 'LGBM': {'macro_f1_folds': array([0.53967094, 0.53721893, 0.5705581 , 0.55926661, 0.53989616]),\n",
       "  'accuracy_test': 0.6120341266277504,\n",
       "  'macro_f1_test': 0.5401368880977374,\n",
       "  'recall_score_test': 0.5339954442463696,\n",
       "  'precision_score_test': 0.5567101855007813},\n",
       " 'XGB': {'macro_f1_folds': array([0.53870017, 0.52842915, 0.55914059, 0.55306228, 0.54256871]),\n",
       "  'accuracy_test': 0.608890884598114,\n",
       "  'macro_f1_test': 0.5453351392513166,\n",
       "  'recall_score_test': 0.5368928386415034,\n",
       "  'precision_score_test': 0.5648767455806984}}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_model_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f6283801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_peformance_to_csv(file_name, header, report_dict):\n",
    "    #header = ['ModelName','Accuracy','MacroF1', 'Recall','Precision']\n",
    "    with open(file_name,'w',encoding='utf-8',newline=\"\") as file:\n",
    "        csvWriter = csv.writer(file)\n",
    "    \n",
    "        csvWriter.writerow(header)\n",
    "    \n",
    "        for name in report_dict:\n",
    "            temp_data = []\n",
    "            temp_data.append(name)\n",
    "            for p in report_dict[name]:\n",
    "                if p != 'macro_f1_folds':\n",
    "                    temp_data.append(report_dict[name][p])\n",
    "            csvWriter.writerow(temp_data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5e4ff4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "header = ['name','accuracy_test','macro_f1_test','recall_score_test','precision_score_test']\n",
    "write_peformance_to_csv('classicML_performance_w2v.csv',header,all_model_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db93094e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "badeff87",
   "metadata": {},
   "source": [
    "# Run and train model on TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8f3d146",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_train, X_tfidf_test, y_tfidf_train, y_tfidf_test =split_train_test(x_tfidf_vectors,drop_duplicate_df['HeadText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "937a3b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8908, 27264)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_tfidf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b8e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on the test dataset:  0.6695105523125281\n",
      "Recall score on the test dataset: 0.6126737616792765\n",
      "Precision score on the test dataset: 0.6172534675567778\n",
      "F1 score on the test dataset: 0.6141315546460848\n",
      "[[477   8  13  73  47  27]\n",
      " [  5 188   2   2  41  26]\n",
      " [ 25   9 414   3  23  13]\n",
      " [ 97   0   5  51  11   0]\n",
      " [ 52  55  12   8 248  67]\n",
      " [ 32  10   4   5  61 113]]\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "             facts       0.69      0.74      0.72       645\n",
      "            issues       0.70      0.71      0.70       264\n",
      " oder and decision       0.92      0.85      0.88       487\n",
      "procedural history       0.36      0.31      0.33       164\n",
      "reasoning/analysis       0.58      0.56      0.57       442\n",
      "             rules       0.46      0.50      0.48       225\n",
      "\n",
      "          accuracy                           0.67      2227\n",
      "         macro avg       0.62      0.61      0.61      2227\n",
      "      weighted avg       0.67      0.67      0.67      2227\n",
      "\n",
      "LR : 0.6093060907629764 (0.014564125424973737)\n",
      "Accuracy score on the test dataset:  0.6650202065559048\n",
      "Recall score on the test dataset: 0.5647124364671303\n",
      "Precision score on the test dataset: 0.5964837804201594\n",
      "F1 score on the test dataset: 0.5497908901909591\n",
      "[[537  21  28   7  39  13]\n",
      " [  8 202   9   0  29  16]\n",
      " [ 22  13 431   1  19   1]\n",
      " [144   3   4   6   6   1]\n",
      " [ 56  82  30   1 223  50]\n",
      " [ 43  17   9   0  74  82]]\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "             facts       0.66      0.83      0.74       645\n",
      "            issues       0.60      0.77      0.67       264\n",
      " oder and decision       0.84      0.89      0.86       487\n",
      "procedural history       0.40      0.04      0.07       164\n",
      "reasoning/analysis       0.57      0.50      0.54       442\n",
      "             rules       0.50      0.36      0.42       225\n",
      "\n",
      "          accuracy                           0.67      2227\n",
      "         macro avg       0.60      0.56      0.55      2227\n",
      "      weighted avg       0.64      0.67      0.64      2227\n",
      "\n",
      "RF : 0.5552399180257934 (0.012005313319345219)\n",
      "Accuracy score on the test dataset:  0.18545127974854064\n",
      "Recall score on the test dataset: 0.2286796159951305\n",
      "Precision score on the test dataset: 0.5216296933384627\n",
      "F1 score on the test dataset: 0.14731054196897658\n",
      "[[  3 631   2   7   0   2]\n",
      " [  0 256   0   0   2   6]\n",
      " [  0 354 125   5   1   2]\n",
      " [  0 153   1  10   0   0]\n",
      " [  1 428   1   3   2   7]\n",
      " [  0 207   0   1   0  17]]\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "             facts       0.75      0.00      0.01       645\n",
      "            issues       0.13      0.97      0.22       264\n",
      " oder and decision       0.97      0.26      0.41       487\n",
      "procedural history       0.38      0.06      0.11       164\n",
      "reasoning/analysis       0.40      0.00      0.01       442\n",
      "             rules       0.50      0.08      0.13       225\n",
      "\n",
      "          accuracy                           0.19      2227\n",
      "         macro avg       0.52      0.23      0.15      2227\n",
      "      weighted avg       0.60      0.19      0.14      2227\n",
      "\n",
      "KNN : 0.1971859216869677 (0.07787936714571529)\n",
      "Accuracy score on the test dataset:  0.5388414907947912\n",
      "Recall score on the test dataset: 0.49166594893578414\n",
      "Precision score on the test dataset: 0.48421304630730533\n",
      "F1 score on the test dataset: 0.4868937455373286\n",
      "[[362  37  23 111  70  42]\n",
      " [ 17 154  16   4  47  26]\n",
      " [ 32  21 374   7  35  18]\n",
      " [100   6   5  37  11   5]\n",
      " [ 67  82  35   8 184  66]\n",
      " [ 33   9  11  18  65  89]]\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "             facts       0.59      0.56      0.58       645\n",
      "            issues       0.50      0.58      0.54       264\n",
      " oder and decision       0.81      0.77      0.79       487\n",
      "procedural history       0.20      0.23      0.21       164\n",
      "reasoning/analysis       0.45      0.42      0.43       442\n",
      "             rules       0.36      0.40      0.38       225\n",
      "\n",
      "          accuracy                           0.54      2227\n",
      "         macro avg       0.48      0.49      0.49      2227\n",
      "      weighted avg       0.55      0.54      0.54      2227\n",
      "\n",
      "DT : 0.4832589606604455 (0.00660489429034053)\n",
      "Accuracy score on the test dataset:  0.3709025594970813\n",
      "Recall score on the test dataset: 0.3170335017860407\n",
      "Precision score on the test dataset: 0.33332573916364716\n",
      "F1 score on the test dataset: 0.3196031982717134\n",
      "[[364  23  65  39 121  33]\n",
      " [ 59  95  14   3  75  18]\n",
      " [ 94  83 169  19  89  33]\n",
      " [ 79   3  14  17  36  15]\n",
      " [148  48  62  11 127  46]\n",
      " [ 76  15  17   4  59  54]]\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "             facts       0.44      0.56      0.50       645\n",
      "            issues       0.36      0.36      0.36       264\n",
      " oder and decision       0.50      0.35      0.41       487\n",
      "procedural history       0.18      0.10      0.13       164\n",
      "reasoning/analysis       0.25      0.29      0.27       442\n",
      "             rules       0.27      0.24      0.25       225\n",
      "\n",
      "          accuracy                           0.37      2227\n",
      "         macro avg       0.33      0.32      0.32      2227\n",
      "      weighted avg       0.37      0.37      0.36      2227\n",
      "\n",
      "NB : 0.3228230378208909 (0.01168873455731419)\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "models.append(('LR', LogisticRegression(solver='lbfgs', max_iter=700)))\n",
    "models.append(('RF', RandomForestClassifier()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('DT', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC(decision_function_shape='ovo', probability=True)))\n",
    "models.append(('LGBM', LGBMClassifier(objective= 'binary')))\n",
    "models.append(('XGB', XGBClassifier(eval_metric=\"mlogloss\", objective = \"reg:logistic\") ))\n",
    "\n",
    "f1_score_folds = {}\n",
    "accuracy_scores = {}\n",
    "f1_score_tests = {}\n",
    "all_model_reports = {}\n",
    "\n",
    "for name, model in models:\n",
    "    report = model_score(name, model,X_tfidf_train, X_tfidf_test, y_tfidf_train, y_tfidf_test)\n",
    "    f1_score_folds[name] = report['macro_f1_folds']\n",
    "    accuracy_scores[name] = report['accuracy_test']\n",
    "    f1_score_tests[name] = report['macro_f1_test']\n",
    "    all_model_reports[name] = report\n",
    "    print(\"{name} : {f1_folds_mean} ({std})\".format(name = name,f1_folds_mean =report['macro_f1_folds'].mean(),\n",
    "                                                   std = report['macro_f1_folds'].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a20f2ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on the test dataset:  0.6699595868881904\n",
      "Recall score on the test dataset: 0.5468254020690922\n",
      "Precision score on the test dataset: 0.7428890830063071\n",
      "F1 score on the test dataset: 0.5504187844300278\n",
      "[[582   2  11   0  47   3]\n",
      " [ 18 163   1   0  73   9]\n",
      " [ 45   1 401   0  36   4]\n",
      " [147   0   4   4   9   0]\n",
      " [ 89  38  10   0 278  27]\n",
      " [ 65   3   1   0  92  64]]\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "             facts       0.62      0.90      0.73       645\n",
      "            issues       0.79      0.62      0.69       264\n",
      " oder and decision       0.94      0.82      0.88       487\n",
      "procedural history       1.00      0.02      0.05       164\n",
      "reasoning/analysis       0.52      0.63      0.57       442\n",
      "             rules       0.60      0.28      0.39       225\n",
      "\n",
      "          accuracy                           0.67      2227\n",
      "         macro avg       0.74      0.55      0.55      2227\n",
      "      weighted avg       0.71      0.67      0.64      2227\n",
      "\n",
      "SVM : 0.5550170160705152 (0.011613806983318197)\n",
      "Accuracy score on the test dataset:  0.6802873821284239\n",
      "Recall score on the test dataset: 0.6072805337536431\n",
      "Precision score on the test dataset: 0.626403340976502\n",
      "F1 score on the test dataset: 0.6126207267425429\n",
      "[[505  10  15  45  44  26]\n",
      " [  4 181   4   1  56  18]\n",
      " [ 19   6 436   0  22   4]\n",
      " [110   0   3  41   9   1]\n",
      " [ 56  57  24   5 245  55]\n",
      " [ 33  11   7   5  62 107]]\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "             facts       0.69      0.78      0.74       645\n",
      "            issues       0.68      0.69      0.68       264\n",
      " oder and decision       0.89      0.90      0.89       487\n",
      "procedural history       0.42      0.25      0.31       164\n",
      "reasoning/analysis       0.56      0.55      0.56       442\n",
      "             rules       0.51      0.48      0.49       225\n",
      "\n",
      "          accuracy                           0.68      2227\n",
      "         macro avg       0.63      0.61      0.61      2227\n",
      "      weighted avg       0.67      0.68      0.67      2227\n",
      "\n",
      "LGBM : 0.6012601897547398 (0.01056122646413786)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cindy.DESKTOP-P0UU59K\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\cindy.DESKTOP-P0UU59K\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\cindy.DESKTOP-P0UU59K\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\cindy.DESKTOP-P0UU59K\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\cindy.DESKTOP-P0UU59K\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score on the test dataset:  0.6686124831612034\n",
      "Recall score on the test dataset: 0.6021301509520721\n",
      "Precision score on the test dataset: 0.6139833061052088\n",
      "F1 score on the test dataset: 0.6056744502101026\n",
      "[[489  10  13  59  47  27]\n",
      " [  9 181   5   0  54  15]\n",
      " [ 22   7 430   1  23   4]\n",
      " [110   1   2  41   9   1]\n",
      " [ 60  60  25   5 234  58]\n",
      " [ 33  10   8   5  55 114]]\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "             facts       0.68      0.76      0.71       645\n",
      "            issues       0.67      0.69      0.68       264\n",
      " oder and decision       0.89      0.88      0.89       487\n",
      "procedural history       0.37      0.25      0.30       164\n",
      "reasoning/analysis       0.55      0.53      0.54       442\n",
      "             rules       0.52      0.51      0.51       225\n",
      "\n",
      "          accuracy                           0.67      2227\n",
      "         macro avg       0.61      0.60      0.61      2227\n",
      "      weighted avg       0.66      0.67      0.66      2227\n",
      "\n",
      "XGB : 0.5943352777686013 (0.0071973274983053006)\n"
     ]
    }
   ],
   "source": [
    "f1_score_folds = {}\n",
    "accuracy_scores = {}\n",
    "f1_score_tests = {}\n",
    "all_model_reports = {}\n",
    "\n",
    "for name, model in models[5:]:\n",
    "    report = model_score(name, model,X_tfidf_train, X_tfidf_test, y_tfidf_train, y_tfidf_test)\n",
    "    f1_score_folds[name] = report['macro_f1_folds']\n",
    "    accuracy_scores[name] = report['accuracy_test']\n",
    "    f1_score_tests[name] = report['macro_f1_test']\n",
    "    all_model_reports[name] = report\n",
    "    print(\"{name} : {f1_folds_mean} ({std})\".format(name = name,f1_folds_mean =report['macro_f1_folds'].mean(),\n",
    "                                                   std = report['macro_f1_folds'].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37f4ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['name','accuracy_test','macro_f1_test','recall_score_test','precision_score_test']\n",
    "write_peformance_to_csv('classicML_performance_w.csv',header,all_model_reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ba1fb9",
   "metadata": {},
   "source": [
    "# Train and Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae95bf14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfac83f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5b8b21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
